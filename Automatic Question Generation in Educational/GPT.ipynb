{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from langchain_community.embeddings import GPT4AllEmbeddings\n",
    "import faiss\n",
    "from langchain_community.docstore.in_memory import InMemoryDocstore\n",
    "from langchain_community.vectorstores import FAISS\n",
    "\n",
    "embeddings = GPT4AllEmbeddings()\n",
    "index = faiss.IndexFlatL2(len(embeddings.embed_query(\"hello world\")))\n",
    "vector_store = FAISS(\n",
    "    embedding_function=embeddings,\n",
    "    index=index,\n",
    "    docstore=InMemoryDocstore(),\n",
    "    index_to_docstore_id={}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "client = OpenAI(os.getenv(\"OPENAI_API_KEY\"))\n",
    "\n",
    "def ask_gpt(prompt: str) -> str:\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-4o\",\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": prompt,\n",
    "            }\n",
    "        ],\n",
    "        max_tokens=512\n",
    "    )\n",
    "    return response.choices[0].message.content\n",
    "\n",
    "sys_res = \"Given the context, generate a question\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.documents import Document\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langgraph.graph import START, StateGraph\n",
    "from typing_extensions import List, TypedDict\n",
    "from langchain_community.document_loaders.csv_loader import CSVLoader\n",
    "\n",
    "def chunking(sauce: str, chunk_size, chunk_overlap) -> str:\n",
    "    print(\"Begin chunking...\")\n",
    "    loader = CSVLoader(sauce)\n",
    "    docs = loader.load()\n",
    "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=chunk_size, chunk_overlap=chunk_overlap)\n",
    "    print(\"Done!\")\n",
    "    return text_splitter.split_documents(docs)\n",
    "\n",
    "# Path to the saved vector store\n",
    "faiss_index_path = \"faiss_index\"\n",
    "if os.path.exists(faiss_index_path):\n",
    "    vector_store = FAISS.load_local(faiss_index_path, embeddings, allow_dangerous_deserialization=True)\n",
    "else:\n",
    "    all_splits = chunking(\"QAG_Train_wop.csv\", 400, 80)\n",
    "    print(\"Begin vector storing...\")\n",
    "    _ = vector_store.add_documents(documents=all_splits)\n",
    "    print(\"Done vector storing!\")\n",
    "\n",
    "    vector_store.save_local(\"faiss_index\")\n",
    "\n",
    "retriever = vector_store.as_retriever(search_type=\"mmr\", search_kwargs={\"k\": 10})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_res = '''\n",
    "1. Record: {record}\n",
    "2. Reference: {reference}\n",
    "3. Task: {task}\n",
    "4. Answer:\n",
    "'''\n",
    "\n",
    "prompt_key = '''\n",
    "1. Record: {record}\n",
    "2. Task: {task}\n",
    "'''\n",
    "\n",
    "record = '''\n",
    "Patient: John Smith\n",
    "Age: 58, Male\n",
    "Medical History: Diabetes; multivessel coronary artery disease with left anterior descending (LAD) involvement\n",
    "Vital Signs: BP 140/85 mmHg, HR 80 bpm\n",
    "Laboratory Findings: Hemoglobin 9.0 g/dL\n",
    "Preoperative Workup: Basic clinical assessment, coronary angiography\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Compiling...\n",
      "Done compiling!\n"
     ]
    }
   ],
   "source": [
    "log = []\n",
    "\n",
    "# Define state for application\n",
    "class State(TypedDict):\n",
    "    task: str #\n",
    "    record: str #\n",
    "    reference: List[Document]\n",
    "    answer: str\n",
    "\n",
    "# Define application steps\n",
    "def retrieve(state: State):\n",
    "    print(\"Begin retriving!\")\n",
    "    messages = prompt_key.format(record=state[\"record\"], task=state[\"task\"])\n",
    "    keywords = ask_gpt(messages, sys_key)\n",
    "    log.append(\"\\n\\nDebug: Keywords\\n\\n\" + keywords) #\n",
    "    docs = retriever.invoke(keywords)\n",
    "    print(\"Done retrieving!\")\n",
    "    return {\"reference\": docs}\n",
    "\n",
    "def generate(state: State):\n",
    "    print(\"Begin generating!\")\n",
    "    docs_content = \"\\n\\n\".join(doc.page_content for doc in state[\"reference\"])\n",
    "    messages = prompt_res.format(record = state[\"record\"], task = state[\"task\"], reference = docs_content)\n",
    "    log.append(\"\\n\\nDebug: Prompt\\n\\n\" + messages) #\n",
    "    response = ask_gpt(messages, sys_res, \"llama3.3-70b\")\n",
    "    print(\"Done generating!\")\n",
    "    return {\"answer\": response}\n",
    "\n",
    "print(\"Compiling...\")\n",
    "graph_builder = StateGraph(State).add_sequence([retrieve, generate])\n",
    "graph_builder.add_edge(START, \"retrieve\")\n",
    "graph = graph_builder.compile()\n",
    "print(\"Done compiling!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Doing Task 1\n",
      "Begin retriving!\n",
      "Done retrieving!\n",
      "Begin generating!\n",
      "Done generating!\n",
      "Doing Task 2\n",
      "Begin retriving!\n",
      "Done retrieving!\n",
      "Begin generating!\n",
      "Done generating!\n",
      "Doing Task 3\n",
      "Begin retriving!\n",
      "Done retrieving!\n",
      "Begin generating!\n",
      "Done generating!\n",
      "Doing Task 4\n",
      "Begin retriving!\n",
      "Done retrieving!\n",
      "Begin generating!\n",
      "Done generating!\n"
     ]
    }
   ],
   "source": [
    "def logging(taskn: str, recordn: str, cnt: int):\n",
    "    log.clear()\n",
    "    print(f\"Doing Task {cnt}\")\n",
    "    response = graph.invoke({\"task\": taskn, \"record\": recordn})\n",
    "    log.append(\"\\n\\nDebug: Response\\n\\n\" + response[\"answer\"]) #\n",
    "    with open(f\"Result/Task{cnt}.txt\", \"w\", encoding=\"utf-8\") as file:\n",
    "        file.write(\"\\n\".join(log))\n",
    "\n",
    "logging(task1, record, 1)\n",
    "logging(task2, record, 2)\n",
    "logging(task3, record, 3)\n",
    "logging(task4, record, 4)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
