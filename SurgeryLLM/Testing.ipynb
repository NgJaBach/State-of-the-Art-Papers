{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All PDF files downloaded\n"
     ]
    }
   ],
   "source": [
    "# Import libraries\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# URL from which pdfs to be downloaded\n",
    "url = \"https://www.ahajournals.org/doi/pdf/10.1161/CIR.0000000000001038\"\n",
    "\n",
    "# Requests URL and get response object\n",
    "response = requests.get(url)\n",
    "\n",
    "# Parse text obtained\n",
    "soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "# Find all hyperlinks present on webpage\n",
    "links = soup.find_all('a')\n",
    "\n",
    "i = 0\n",
    "\n",
    "# From all links check for pdf link and\n",
    "# if present download file\n",
    "for link in links:\n",
    "\tif ('.pdf' in link.get('href', [])):\n",
    "\t\ti += 1\n",
    "\t\tprint(\"Downloading file: \", i)\n",
    "\n",
    "\t\t# Get response object for link\n",
    "\t\tresponse = requests.get(link.get('href'))\n",
    "\n",
    "\t\t# Write content in pdf file\n",
    "\t\tpdf = open(\"pdf\"+str(i)+\".pdf\", 'wb')\n",
    "\t\tpdf.write(response.content)\n",
    "\t\tpdf.close()\n",
    "\t\tprint(\"File \", i, \" downloaded\")\n",
    "\n",
    "print(\"All PDF files downloaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import getpass\n",
    "import os\n",
    "from langchain_community.embeddings import GPT4AllEmbeddings\n",
    "from langchain_chroma import Chroma\n",
    "\n",
    "gpt4all_embd = GPT4AllEmbeddings()\n",
    "\n",
    "vector_store = Chroma(\n",
    "    collection_name=\"example_collection\",\n",
    "    embedding_function=gpt4all_embd,\n",
    "    persist_directory=\"./chroma_langchain_db\",  # Where to save data locally, remove if not necessary\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "client = OpenAI(\n",
    "    # This is the default and can be omitted\n",
    "    api_key=os.environ.get(\"LLAMA_API_KEY\"),\n",
    "    base_url=\"https://api.llama-api.com/\"\n",
    ")\n",
    "\n",
    "def ask_gpt(prompt) -> str:\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"llama3.3-70b\",\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": prompt,\n",
    "            }\n",
    "        ],\n",
    "        # max_tokens=512,\n",
    "        stream=False\n",
    "    )\n",
    "    return response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "template = \"\" \\\n",
    "\"You are an assistant specialized in question-answering surgical tasks.\" \\\n",
    "\"Use the retrieved context below to provide a concise, clinically accurate answer to the question.\" \\\n",
    "\"Clearly reference relevant guidelines or sources from the context where possible.\" \\\n",
    "\"If you cannot answer based on the provided information, say you don't know.\" \\\n",
    "\"Keep the answer clear and concise, but provide enough detail to be medically useful.\" \\\n",
    "\"Question: {question}\" \\\n",
    "\"Context: {context}\" \\\n",
    "\"Answer:\"\n",
    "\n",
    "task1 = \"Checking patient records for missing clinical investigation data.\"\n",
    "\n",
    "task2 = \"Identifying and flagging investigation results outside of normal ranges.\"\n",
    "\n",
    "task3 = \"Developing recommendations for next management steps based on national surgical guidelines.\"\n",
    "\n",
    "task4 = \"Preparing structured operative notes based upon recommended management steps.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.documents import Document\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langgraph.graph import START, StateGraph\n",
    "from typing_extensions import List, TypedDict\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "\n",
    "# Define state for application\n",
    "class State(TypedDict):\n",
    "    sauce: str\n",
    "    question: str\n",
    "    context: List[Document]\n",
    "    answer: str\n",
    "\n",
    "# Define application steps\n",
    "def indexing(state: State):\n",
    "    # Load and chunk contents of the pdf\n",
    "    loader = PyPDFLoader(state[\"sauce\"])\n",
    "    docs = loader.load()\n",
    "\n",
    "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "    all_splits = text_splitter.split_documents(docs)\n",
    "\n",
    "    # Index chunks\n",
    "    _ = vector_store.add_documents(documents=all_splits)\n",
    "\n",
    "def retrieve(state: State):\n",
    "    retrieved_docs = vector_store.similarity_search(state[\"question\"])\n",
    "    return {\"context\": retrieved_docs}\n",
    "\n",
    "def generate(state: State):\n",
    "    docs_content = \"\\n\\n\".join(doc.page_content for doc in state[\"context\"])\n",
    "    messages = template.invoke(\n",
    "        {\"question\": state[\"question\"], \"context\": docs_content}\n",
    "    ).to_messages()\n",
    "    response = ask_gpt(messages[0].content)\n",
    "    return {\"answer\": response}\n",
    "\n",
    "# Compile application and test\n",
    "graph_builder = StateGraph(State).add_sequence([indexing, retrieve, generate])\n",
    "graph_builder.add_edge(START, \"indexing\")\n",
    "graph = graph_builder.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to multipart ingest runs: langsmith.utils.LangSmithError: Failed to POST https://api.smith.langchain.com/runs/multipart in LangSmith API. HTTPError('403 Client Error: Forbidden for url: https://api.smith.langchain.com/runs/multipart', '{\"error\":\"Forbidden\"}\\n')\n"
     ]
    }
   ],
   "source": [
    "response = graph.invoke({\"sauce\": \"task1.pdf\"}, {\"question\": task1})\n",
    "print(response[\"answer\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
