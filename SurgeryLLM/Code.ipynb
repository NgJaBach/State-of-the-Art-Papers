{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from langchain_community.embeddings import GPT4AllEmbeddings\n",
    "from langchain_chroma import Chroma\n",
    "\n",
    "gpt4all_embd = GPT4AllEmbeddings()\n",
    "\n",
    "vector_store = Chroma(\n",
    "    collection_name=\"mmb\",\n",
    "    embedding_function=gpt4all_embd,\n",
    "    # persist_directory=\"./chroma_langchain_db\",  # Where to save data locally, remove if not necessary\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "client = OpenAI(\n",
    "    # This is the default and can be omitted\n",
    "    api_key=os.environ.get(\"LLAMA_API_KEY\"),\n",
    "    base_url=\"https://api.llama-api.com/\"\n",
    ")\n",
    "\n",
    "def ask_gpt(prompt) -> str:\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"llama3.2-3b\",\n",
    "        # model=\"llama3.3-70b\",\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": prompt,\n",
    "            }\n",
    "        ],\n",
    "        max_tokens=512,\n",
    "        # stream=False\n",
    "    )\n",
    "    return response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "template = '''\n",
    "You are an assistant specialized in answering surgical tasks.\n",
    "Use the retrieved context below to provide a concise, clinically accurate answer to the question.\n",
    "Clearly reference relevant guidelines or sources from the context where possible.\n",
    "If you cannot answer based on the provided information, say you don't know.\n",
    "Keep the answer clear and concise, but provide enough detail to be medically useful.\n",
    "Record: {record}\n",
    "Task: {task}\n",
    "Guideline: {guideline}\n",
    "Answer:\n",
    "'''\n",
    "\n",
    "task1 = \"Identify laboratory results outside of normal reference ranges\"\n",
    "\n",
    "task2 = \"Identify unavailable preoperative tests\"\n",
    "\n",
    "task3 = \"Surgical recommendation\"\n",
    "\n",
    "task4 = \"Prepare sample operative notes.\"\n",
    "\n",
    "record = '''\n",
    "Patient Name: Mr. John Smith  \n",
    "Age: 65 years  \n",
    "Gender: Male  \n",
    "Medical History:\n",
    "- Type 2 Diabetes Mellitus  \n",
    "- Hypertension  \n",
    "- Known multivessel coronary artery disease with involvement of the left anterior descending artery\n",
    "\n",
    "Presenting Clinical Data:  \n",
    "- Laboratory Results: \n",
    "  - Hemoglobin: 9.0 g/dL\n",
    "- Preoperative Evaluation:\n",
    "  - Incomplete assessment\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.documents import Document\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langgraph.graph import START, StateGraph\n",
    "from typing_extensions import List, TypedDict\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Define state for application\n",
    "class State(TypedDict):\n",
    "    sauce: str #\n",
    "    task: str #\n",
    "    record: str #\n",
    "    guideline: List[Document]\n",
    "    answer: str\n",
    "\n",
    "# Define application steps\n",
    "def retrieve(state: State):\n",
    "    # Load and chunk contents of the pdf\n",
    "    loader = PyPDFLoader(state[\"sauce\"])\n",
    "    docs = loader.load()\n",
    "\n",
    "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "    all_splits = text_splitter.split_documents(docs)\n",
    "\n",
    "    # Index chunks\n",
    "    _ = vector_store.add_documents(documents=all_splits)\n",
    "\n",
    "    retrieved_docs = vector_store.similarity_search(state[\"task\"])\n",
    "    return {\"guideline\": retrieved_docs}\n",
    "\n",
    "def generate(state: State):\n",
    "    docs_content = \"\\n\\n\".join(doc.page_content for doc in state[\"guideline\"])\n",
    "    messages = template.format(record=state[\"record\"], task=state[\"task\"], guideline=state[\"guideline\"])\n",
    "    response = ask_gpt(messages)\n",
    "    return {\"answer\": response}\n",
    "\n",
    "# Compile application and test\n",
    "graph_builder = StateGraph(State).add_sequence([retrieve, generate])\n",
    "graph_builder.add_edge(START, \"retrieve\")\n",
    "graph = graph_builder.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Based on the provided context, I don't know which laboratory results are outside of normal reference ranges for Mr. John Smith, as the context does not provide the normal reference ranges for the patient's laboratory results.\n"
     ]
    }
   ],
   "source": [
    "params = {\"sauce\": \"task1.pdf\", \"task\": task1, \"record\": record}\n",
    "response = graph.invoke(params)\n",
    "print(response[\"answer\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
