{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'langchain_community'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mos\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mlangchain_community\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01membeddings\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m GPT4AllEmbeddings\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mfaiss\u001b[39;00m\n\u001b[32m      4\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mlangchain_community\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdocstore\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01min_memory\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m InMemoryDocstore\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'langchain_community'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from langchain_community.embeddings import GPT4AllEmbeddings\n",
    "import faiss\n",
    "from langchain_community.docstore.in_memory import InMemoryDocstore\n",
    "from langchain_community.vectorstores import FAISS\n",
    "\n",
    "embeddings = GPT4AllEmbeddings()\n",
    "index = faiss.IndexFlatL2(len(embeddings.embed_query(\"hello world\")))\n",
    "vector_store = FAISS(\n",
    "    embedding_function=embeddings,\n",
    "    index=index,\n",
    "    docstore=InMemoryDocstore(),\n",
    "    index_to_docstore_id={}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "client = OpenAI(\n",
    "    # This is the default and can be omitted\n",
    "    api_key=os.environ.get(\"LLAMA_API_KEY\"),\n",
    "    base_url=\"https://api.llama-api.com/\"\n",
    ")\n",
    "\n",
    "def ask_gpt(prompt: str, system: str, model=\"llama3.2-3b\") -> str:\n",
    "    response = client.chat.completions.create(\n",
    "        model=model,\n",
    "        # model=\"llama3.2-3b\",\n",
    "        # model=\"llama3.3-70b\",\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"system\", \n",
    "                \"content\": system\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": prompt,\n",
    "            }\n",
    "        ],\n",
    "        max_tokens=512\n",
    "    )\n",
    "    return response.choices[0].message.content\n",
    "\n",
    "sys_res = \"Read patient record and the reference, answer the task concisely. If you don't know, just say it.\"\n",
    "\n",
    "sys_key = \"Summarize the texts to keypoints concisely, say nothing else.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.documents import Document\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langgraph.graph import START, StateGraph\n",
    "from typing_extensions import List, TypedDict\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "\n",
    "def chunking(sauce: str, chunk_size, chunk_overlap) -> str:\n",
    "    print(\"Begin chunking...\")\n",
    "    loader = PyPDFLoader(sauce)\n",
    "    docs = loader.load()\n",
    "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=chunk_size, chunk_overlap=chunk_overlap)\n",
    "    print(\"Done!\")\n",
    "    return text_splitter.split_documents(docs)\n",
    "\n",
    "\n",
    "# Path to the saved vector store\n",
    "faiss_index_path = \"faiss_index\"\n",
    "if os.path.exists(faiss_index_path):\n",
    "    vector_store = FAISS.load_local(faiss_index_path, embeddings, allow_dangerous_deserialization=True)\n",
    "else:\n",
    "    all_splits = []\n",
    "    all_splits = all_splits + chunking(\"ExternalDoc/labval.pdf\", 1000, 20)\n",
    "    all_splits = all_splits + chunking(\"ExternalDoc/aortic.pdf\", 2000, 200)\n",
    "    all_splits = all_splits + chunking(\"ExternalDoc/coronary.pdf\", 2000, 200)\n",
    "    all_splits = all_splits + chunking(\"ExternalDoc/valvular.pdf\", 2000, 200)\n",
    "\n",
    "    print(\"Begin vector storing...\")\n",
    "    _ = vector_store.add_documents(documents=all_splits)\n",
    "    print(\"Done vector storing!\")\n",
    "\n",
    "    vector_store.save_local(\"faiss_index\")\n",
    "\n",
    "retriever = vector_store.as_retriever(search_type=\"mmr\", search_kwargs={\"k\": 10})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_res = '''\n",
    "1. Record: {record}\n",
    "2. Reference: {reference}\n",
    "3. Task: {task}\n",
    "4. Answer:\n",
    "'''\n",
    "\n",
    "prompt_key = '''\n",
    "1. Record: {record}\n",
    "2. Task: {task}\n",
    "'''\n",
    "\n",
    "task1 = \"Check the record and identify results outside of reference ranges.\"\n",
    "task2 = \"Identify unavailable preoperative tests.\"\n",
    "task3 = \"Surgical recommendation.\"\n",
    "task4 = \"Prepare sample operative notes.\"\n",
    "\n",
    "record = '''\n",
    "Patient: John Smith\n",
    "Age: 58, Male\n",
    "Medical History: Diabetes; multivessel coronary artery disease with left anterior descending (LAD) involvement\n",
    "Vital Signs: BP 140/85 mmHg, HR 80 bpm\n",
    "Laboratory Findings: Hemoglobin 9.0 g/dL\n",
    "Preoperative Workup: Basic clinical assessment, coronary angiography\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log = []\n",
    "\n",
    "# Define state for application\n",
    "class State(TypedDict):\n",
    "    task: str #\n",
    "    record: str #\n",
    "    reference: List[Document]\n",
    "    answer: str\n",
    "\n",
    "# Define application steps\n",
    "def retrieve(state: State):\n",
    "    print(\"Begin retriving!\")\n",
    "    messages = prompt_key.format(record=state[\"record\"], task=state[\"task\"])\n",
    "    keywords = ask_gpt(messages, sys_key)\n",
    "    log.append(\"\\n\\nDebug: Keywords\\n\\n\" + keywords) #\n",
    "    docs = retriever.invoke(keywords)\n",
    "    print(\"Done retrieving!\")\n",
    "    return {\"reference\": docs}\n",
    "\n",
    "def generate(state: State):\n",
    "    print(\"Begin generating!\")\n",
    "    docs_content = \"\\n\\n\".join(doc.page_content for doc in state[\"reference\"])\n",
    "    messages = prompt_res.format(record = state[\"record\"], task = state[\"task\"], reference = docs_content)\n",
    "    log.append(\"\\n\\nDebug: Prompt\\n\\n\" + messages) #\n",
    "    response = ask_gpt(messages, sys_res, \"llama3.3-70b\")\n",
    "    print(\"Done generating!\")\n",
    "    return {\"answer\": response}\n",
    "\n",
    "print(\"Compiling...\")\n",
    "graph_builder = StateGraph(State).add_sequence([retrieve, generate])\n",
    "graph_builder.add_edge(START, \"retrieve\")\n",
    "graph = graph_builder.compile()\n",
    "print(\"Done compiling!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logging(taskn: str, recordn: str, cnt: int):\n",
    "    log.clear()\n",
    "    print(f\"Doing Task {cnt}\")\n",
    "    response = graph.invoke({\"task\": taskn, \"record\": recordn})\n",
    "    log.append(\"\\n\\nDebug: Response\\n\\n\" + response[\"answer\"]) #\n",
    "    with open(f\"Result/Task{cnt}.txt\", \"w\", encoding=\"utf-8\") as file:\n",
    "        file.write(\"\\n\".join(log))\n",
    "\n",
    "logging(task1, record, 1)\n",
    "logging(task2, record, 2)\n",
    "logging(task3, record, 3)\n",
    "logging(task4, record, 4)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
